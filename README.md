# RAG-Financial-Statement â€” Additional Notes

This project is based on [InterOpera-Apps/coding-test-2nd](https://github.com/InterOpera-Apps/coding-test-2nd).

**For setup, usage, and general documentation, please refer to the [original repository](https://github.com/InterOpera-Apps/coding-test-2nd).**  
Below are the additional features and differences in this repository:

- **Project Structure**
```
RAG-Financial-Statement/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py                 # FastAPI application
â”‚ â”œâ”€â”€ models/                 # Data models
â”‚ â”‚ â””â”€â”€ schemas.py            # Pydantic schemas
â”‚ â”œâ”€â”€ services/               # RAG service logic
â”‚ â”‚ â”œâ”€â”€ pdf_processor.py      # PDF processing and chunking
â”‚ â”‚ â”œâ”€â”€ providers.py          # Embeddings and LLM management (HF & OpenAI)
â”‚ â”‚ â”œâ”€â”€ vector_store.py       # Vector database integration
â”‚ â”‚ â””â”€â”€ rag_pipeline.py       # RAG pipeline
â”‚ â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚ â””â”€â”€ config.py               # Configuration file
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ pages/                  # Next.js pages
â”‚ â”‚ â”œâ”€â”€ index.tsx             # Main page
â”‚ â”‚ â””â”€â”€ _app.tsx              # App component
â”‚ â”œâ”€â”€ components/             # React components
â”‚ â”‚ â”œâ”€â”€ ChatInterface.tsx
â”‚ â”‚ â””â”€â”€ FileUpload.tsx
â”‚ â”œâ”€â”€ hooks/                        # Hooks
â”‚ â”‚ â””â”€â”€ useKnowledgeBase.ts         # Trigger based on batch process
â”‚ â”œâ”€â”€ styles/                       # CSS files
â”‚ â”‚ â””â”€â”€ globals.css                 # Global styles
â”‚ â”‚ â””â”€â”€ ChatInterface.module.css    # Module style
â”‚ â”‚ â””â”€â”€ FileUpload.module.css       # Module style
â”‚ â”œâ”€â”€ public/                       # folder for public usecases like icon
â”‚ â”‚ â””â”€â”€ favicon.png
â”‚ â”œâ”€â”€ package.json                  # Node.js dependencies
â”‚ â”œâ”€â”€ tsconfig.json                 # TypeScript configuration
â”‚ â”œâ”€â”€ next.config.js                # Next.js configuration
â”‚ â”œâ”€â”€ next-env.d.ts                 # Next.js type definitions
â”‚ â””â”€â”€ .eslintrc.json                # ESLint configuration
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample.pdf
â””â”€â”€ README.md
```

## ðŸš© What's Different in This Repo

- **Modular Provider Management (`providers.py`):**
  - Added a `providers.py` file to make embedding and LLM provider (HuggingFace or OpenAI) selection more modular and extensible. You just configure via env variables.

- **Not Using `_retrieve_documents`:**
  - The function `_retrieve_documents` from the original repo is not used here.
  - All retrieval is handled in `vector_store.py` via `similarity_search`, called directly from the RAG pipeline (`generate_answer`). So there's no dependency on that function.

- **Frontend Response Formatting:**
  - Added `react-markdown`, `remark-math`, and `katex` to render model responses with markdown and math formatting, making answers much cleaner and easier to read.

- **Embeddings & LLM Provider Choice:**
  - You can select either HuggingFace or OpenAI for both embeddings and LLM, simply by changing values in the `.env` file (see `.env`). To use OpenAI, supply your own `OPENAI_API_KEY`.
  
  Sample `.env`:
  ```
  OPENAI_API_KEY= """                         # Fill with our own OpenAI API-Key
  VECTOR_DB_PATH="./vector_store"             # A folder to store vectorstore, already a default
  VECTOR_DB_TYPE="chromadb"                   # 'chromadb' also already a default
  PDF_UPLOAD_PATH="../data"                   # A folder to store the uploaded PDF
  LLM_TEMPERATURE=0.1
  MAX_TOKENS=1000
  CHUNK_SIZE=1000
  CHUNK_OVERLAP=200
  RETRIEVAL_K=5
  SIMILARITY_THRESHOLD=0.7
  EMBEDDING_PROVIDER="openai"                 # openai or huggingface
  LLM_PROVIDER="openai"                       # openai or huggingface
  EMBEDDING_MODEL="text-embedding-ada-002"    # text-embedding-ada-002 or model sentence-transformers
  LLM_MODEL="gpt-4-turbo"                     # gpt-3.5-turbo or gpt-4-turbo or distilbert-base-cased-distilled-squad
  ```

- **Better Prompt Engineering:**
  - In `_generate_llm_response` (see `rag_pipeline.py`), the prompt is improved: if an answer can be calculated manually from data in the context, the model is instructed to do the math and show the steps (with LaTeX/markdown), always showing the final result in bold at the top.

- **Knowledge Base State Hook:**
  - Added a `useKnowledgeBase` React hook in the frontend to handle state changes when PDF processing/batching is done, so the chat interface is only enabled when knowledge base is ready.

- **Component-level Styling:**
  - Styling for each React component (chat, file upload, etc) is implemented as a CSS module for better maintainability.

---

> Again: for overall instructions, setup, and original API routes, see the [main repo documentation](https://github.com/InterOpera-Apps/coding-test-2nd).