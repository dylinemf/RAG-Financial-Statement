# RAG-Financial-Statement â€” Additional Notes

This project is based on [InterOpera-Apps/coding-test-2nd](https://github.com/InterOpera-Apps/coding-test-2nd).

**For setup, usage, and general documentation, please refer to the [original repository](https://github.com/InterOpera-Apps/coding-test-2nd).**  
Below are the additional features and differences in this repository:

- **Project Structure**
```
RAG-Financial-Statement/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py # FastAPI application
â”‚ â”œâ”€â”€ models/ # Data models
â”‚ â”‚ â””â”€â”€ schemas.py # Pydantic schemas
â”‚ â”œâ”€â”€ services/ # RAG service logic
â”‚ â”‚ â”œâ”€â”€ pdf_processor.py # PDF processing and chunking
â”‚ â”‚ â”œâ”€â”€ providers.py # Embeddings and LLM management (HF & OpenAI)
â”‚ â”‚ â”œâ”€â”€ vector_store.py # Vector database integration
â”‚ â”‚ â””â”€â”€ rag_pipeline.py # RAG pipeline
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies
â”‚ â””â”€â”€ config.py # Configuration file
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ pages/ # Next.js pages
â”‚ â”‚ â”œâ”€â”€ index.tsx # Main page
â”‚ â”‚ â””â”€â”€ _app.tsx # App component
â”‚ â”œâ”€â”€ components/ # React components
â”‚ â”‚ â”œâ”€â”€ ChatInterface.tsx
â”‚ â”‚ â””â”€â”€ FileUpload.tsx
â”‚ â”œâ”€â”€ hooks/ # Hooks
â”‚ â”‚ â””â”€â”€ useKnowledgeBase.ts # Trigger based on batch process
â”‚ â”œâ”€â”€ styles/ # CSS files
â”‚ â”‚ â””â”€â”€ globals.css # Global styles
â”‚ â”‚ â””â”€â”€ ChatInterface.module.css # Module style
â”‚ â”‚ â””â”€â”€ FileUpload.module.css # Module style
â”‚ â”œâ”€â”€ public/ # Hooks
â”‚ â”‚ â””â”€â”€ favicon.png
â”‚ â”œâ”€â”€ package.json # Node.js dependencies
â”‚ â”œâ”€â”€ tsconfig.json # TypeScript configuration
â”‚ â”œâ”€â”€ next.config.js # Next.js configuration
â”‚ â”œâ”€â”€ next-env.d.ts # Next.js type definitions
â”‚ â””â”€â”€ .eslintrc.json # ESLint configuration
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample.pdf
â””â”€â”€ README.md
```

## ðŸš© What's Different in This Repo

- **Modular Provider Management (`providers.py`):**
  - Added a `providers.py` file to make embedding and LLM provider (HuggingFace or OpenAI) selection more modular and extensible. You just configure via env variables.

- **Not Using `_retrieve_documents`:**
  - The function `_retrieve_documents` from the original repo is not used here.
  - All retrieval is handled in `vector_store.py` via `similarity_search`, called directly from the RAG pipeline (`generate_answer`). So there's no dependency on that function.

- **Frontend Response Formatting:**
  - Added `react-markdown`, `remark-math`, and `katex` to render model responses with markdown and math formatting, making answers much cleaner and easier to read.

- **Embeddings & LLM Provider Choice:**
  - You can select either HuggingFace or OpenAI for both embeddings and LLM, simply by changing values in the `.env` file (see `.env.sample`). To use OpenAI, supply your own `OPENAI_API_KEY`.

- **Better Prompt Engineering:**
  - In `_generate_llm_response` (see `rag_pipeline.py`), the prompt is improved: if an answer can be calculated manually from data in the context, the model is instructed to do the math and show the steps (with LaTeX/markdown), always showing the final result in bold at the top.

- **Knowledge Base State Hook:**
  - Added a `useKnowledgeBase` React hook in the frontend to handle state changes when PDF processing/batching is done, so the chat interface is only enabled when knowledge base is ready.

- **Component-level Styling:**
  - Styling for each React component (chat, file upload, etc) is implemented as a CSS module for better maintainability.

---

> Again: for overall instructions, setup, and original API routes, see the [main repo documentation](https://github.com/InterOpera-Apps/coding-test-2nd).